apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    serving.knative.openshift.io/enablePassthrough: 'true'
    sidecar.istio.io/inject: 'true'
    sidecar.istio.io/rewriteAppHTTPProbers: 'true'
  name: {{ include "vllm.fullname" . }}
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    maxReplicas: {{ .Values.maxReplicas | default 1 }}
    minReplicas: {{ .Values.minReplicas }}
    model:
      modelFormat:
        name: vLLM
      name: ''
      args:
        {{- toYaml .Values.inferenceService.args | nindent 8 }}
      resources: 
        {{- .Values.servingModel.resources | toYaml | nindent 8 }}
      runtime: {{ include "vllm.fullname" . }}
      storageUri: {{ .Values.runtime.storageUri }}
      {{- with .Values.tolerations }}
    tolerations:
      {{- toYaml . | nindent 6 }}
    {{- end }}
