fullnameOverride: "vllm"

inferenceSerivce:
  storage:
    storageUri: 'oci://quay.io/redhat-ai-services/modelcar-catalog:mistral-7b-instruct-v0.3'
  args: []
  resources:
    limits:
      cpu: '4'
      memory: 20Gi
      nvidia.com/gpu: '1'
    requests:
      cpu: '2'
      memory: 16Gi
      nvidia.com/gpu: '1'
  tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Equal
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Equal
    value: "True"
